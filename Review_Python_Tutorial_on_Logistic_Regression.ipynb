{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1SMkMY8WlP7iZlSMaZfey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jman4162/machine-learning-review/blob/main/Review_Python_Tutorial_on_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review: Python Tutorial on Logistic Regression\n",
        "\n",
        "Name: John Hodge\n",
        "\n",
        "Date: 05/09/24"
      ],
      "metadata": {
        "id": "3MtecYdWWpe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Logistic regression is a statistical model used for binary classification tasks, which predicts the probability that an observation falls into one of two categories. This makes it particularly useful in fields like medicine, finance, and social sciences for outcomes like disease/no disease, default/no default, etc. Unlike linear regression, logistic regression outputs probabilities by applying a logistic function to a linear combination of features."
      ],
      "metadata": {
        "id": "nWBIfu9NWvNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory Behind Logistic Regression\n",
        "\n",
        "### The Logistic Function\n",
        "\n",
        "The logistic function, also known as the sigmoid function, is defined as:\n",
        "\n",
        "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "Here, \\( $z$ \\) is the linear combination of the input features ($x$), represented as:\n",
        "\n",
        "$ z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n $\n",
        "\n",
        "Where \\( $\\beta_0, \\beta_1, ..., \\beta_n$ \\) are the coefficients of the model.\n",
        "\n",
        "### Probabilities and Odds Ratios\n",
        "\n",
        "The logistic function outputs values between 0 and 1, which are interpreted as probabilities. The odds ratio for a variable can be given by the exponential of its coefficient:\n",
        "\n",
        "$ \\text{Odds Ratio} = e^{\\beta_i} $\n",
        "\n",
        "This indicates how a unit increase in the variable \\( $x_i$ \\) affects the odds of the outcome occurring.\n",
        "\n",
        "### Cost Function: Log Loss\n",
        "\n",
        "To train a logistic regression model, we minimize a cost function called \"log loss\", which is defined for a single observation as:\n",
        "\n",
        "$ \\text{Log Loss} = -y \\log(p) - (1-y) \\log(1-p) $\n",
        "\n",
        "Where \\( $y$ \\) is the actual label (0 or 1) and \\( $p$ \\) is the predicted probability of the outcome being 1. See the Appendix for a detailed explanation and derivation of the log-loss function.\n",
        "\n"
      ],
      "metadata": {
        "id": "9bmHhF38W1IW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Logistic Regression in Python\n",
        "\n",
        "We'll use the popular `scikit-learn` library to implement logistic regression. Let's start by setting up our environment and loading some data to work with.\n",
        "\n",
        "### Setup and Data Loading"
      ],
      "metadata": {
        "id": "Ihpgv8vTXNDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xTtRWJr6Wo15"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data set\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)"
      ],
      "metadata": {
        "id": "sEAQoiw1X6UG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data\n",
        "\n",
        "Before training, we split our dataset into training and testing sets:"
      ],
      "metadata": {
        "id": "4gpMvSJMXdOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "xuroL0TsXn5T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "\n",
        "Now, let's train our logistic regression model:"
      ],
      "metadata": {
        "id": "SJOVBO4qXoXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "h2aOsEfIXqUf",
        "outputId": "e5fde140-c9b9-4487-d65c-32638d20a1cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "After training the model, we can evaluate its performance on the test set:"
      ],
      "metadata": {
        "id": "YdQQYXVqXq1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print out the confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2zHZRs8XsYB",
        "outputId": "f9bdf5cb-c376-4e8f-fa3f-7be24add8e28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[104  11]\n",
            " [ 26 109]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85       115\n",
            "           1       0.91      0.81      0.85       135\n",
            "\n",
            "    accuracy                           0.85       250\n",
            "   macro avg       0.85      0.86      0.85       250\n",
            "weighted avg       0.86      0.85      0.85       250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting Coefficients\n",
        "\n",
        "We can also look at the coefficients of the model to interpret the impact of each feature:"
      ],
      "metadata": {
        "id": "Gk6Ah5kbXs0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficients and intercept\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nLivgXgXuYH",
        "outputId": "5452957e-1eed-4965-8251-d30bfd06b08c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: [0.12283339]\n",
            "Coefficients: [[ 0.10919003 -0.49578821  0.16607838  0.14131473  0.05205305  1.59487115\n",
            "  -0.0959503   0.0280646  -0.02121372  0.01935852  0.1831346   0.43485908\n",
            "   0.03552626  0.16072815 -0.49521638  0.11335956  0.06258129 -0.13471173\n",
            "  -0.75742756  0.1332694 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results from the logistic regression model consist of an intercept and an array of coefficients, which are crucial for understanding how the model makes predictions. Here's a breakdown of what these values represent and how they impact the prediction:\n",
        "\n",
        "### Intercept: \\([0.12283339]\\)\n",
        "- **Intercept (or Bias)**: The intercept in logistic regression represents the log-odds of the outcome when all the predictor variables \\( $X_i$ \\) are set to zero. In more practical terms, it's a baseline prediction when no other information (features) is provided. In your model, the intercept is approximately 0.123, suggesting that when \\( $X = 0$ \\), the log-odds of the positive class is slightly positive.\n",
        "\n",
        "### Coefficients:\n",
        "- **Coefficients**: These values represent the change in the log-odds of the dependent variable \\( $Y$ \\) (i.e., the probability of the positive outcome) for a one-unit change in the corresponding independent variable \\( $X_i$ \\), while all other variables remain constant. These coefficients are crucial for understanding the influence of each feature on the outcome. Here’s a brief look at the role of each coefficient:\n",
        "\n",
        "    - \\( $\\beta_1 = 0.109$ \\): A positive coefficient indicates that as this feature increases by one unit, the log-odds of the positive class increases by approximately 0.109, making the event more likely.\n",
        "    - \\( $\\beta_2 = -0.496$ \\): A negative coefficient means that increasing this feature by one unit decreases the log-odds of the positive class by about 0.496, making the event less likely.\n",
        "    - \\( $\\beta_6 = 1.595$ \\): This feature has a significant positive impact on the probability of the outcome being positive. Its high coefficient suggests it's a strong predictor in favor of the positive class.\n",
        "    - \\( $\\beta_{19} = -0.757$ \\): This feature strongly decreases the probability of a positive outcome, as indicated by its substantial negative coefficient.\n",
        "  \n",
        "### Understanding Log-Odds:\n",
        "- **Log-Odds**: The logistic regression model calculates the probability of the dependent variable being 1 (positive class) as the sigmoid function of the linear combination of features. Mathematically, it's expressed as:\n",
        "\n",
        "  $ p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}} $\n",
        "  \n",
        "  Here, \\( $\\beta_0, \\beta_1, ..., \\beta_n$ \\) are your intercept and coefficients, respectively, and \\( $x_1, x_2, ..., x_n$ \\) are your feature values.\n",
        "\n",
        "### Implications:\n",
        "- Each coefficient tells us how much weight or importance the model gives to each feature when predicting the outcome.\n",
        "- Positive coefficients increase the probability of the positive class as the value of the corresponding feature increases, while negative coefficients decrease it.\n",
        "\n",
        "These coefficients and the intercept form the backbone of the logistic regression model, allowing it to estimate probabilities based on a combination of these inputs. Understanding these can help interpret the model in the context of the specific application, whether it’s in predicting medical outcomes, customer behavior, or any other binary classification task."
      ],
      "metadata": {
        "id": "fgtXxB-vYm8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This tutorial introduced the basics of logistic regression, from its theoretical foundations to its implementation in Python using `scikit-learn`. This model is powerful for binary classification problems, providing probabilities and classifications based on the input features.\n",
        "\n",
        "By analyzing the coefficients and the model's performance metrics, you can gain insights into the relationships between features and the predicted outcome, allowing for better-informed decision-making in practical applications."
      ],
      "metadata": {
        "id": "AZqIwoldXu2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "## Deriving the Log-Loss Function\n",
        "\n",
        "The log-loss function, also known as binary cross-entropy loss, is commonly used in binary classification tasks to measure the performance of a classification model whose output is a probability value between 0 and 1. The log-loss provides a measure of accuracy where the prediction input is a probability value between 0 (representing class 0) and 1 (representing class 1).\n",
        "\n",
        "### Theoretical Foundation\n",
        "\n",
        "The log-loss function is derived from the concept of likelihood in statistical modeling, particularly from maximizing the likelihood of the observed data under the model.\n",
        "\n",
        "#### 1. Model Prediction:\n",
        "Consider a binary classification problem where each instance \\( $x_i$ \\) is predicted to belong to one of two classes (0 or 1) with probability \\( $p_i$ \\), which is the model's estimated probability that $y_i = 1$. The probability that $y_i = 0$ is \\( $1 - p_i$ \\).\n",
        "\n",
        "#### 2. Likelihood of an Observation:\n",
        "For a single observation \\( $x_i, y_i$ \\), the likelihood of observing \\( $y_i$ \\), given \\( $p_i$ \\), is:\n",
        "\n",
        "$ P(y_i | p_i) = p_i^{y_i} (1 - p_i)^{(1 - y_i)} $\n",
        "\n",
        "This equation states that:\n",
        "- If \\( $y_i = 1$ \\), the likelihood is \\( $p_i$ \\).\n",
        "- If \\( $y_i = 0$ \\), the likelihood is \\( $1 - p_i$ \\).\n",
        "\n",
        "#### 3. Log-Likelihood:\n",
        "To simplify calculations and manage numerical underflow, we use the logarithm of the likelihood. For a single observation, the log-likelihood is:\n",
        "\n",
        "$ \\log P(y_i | p_i) = y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) $\n",
        "\n",
        "#### 4. Negative Log-Likelihood for All Observations:\n",
        "When considering the entire dataset of \\( $n$ \\) observations, the log-likelihood of the dataset is the sum of the log-likelihoods of all individual observations. The negative log-likelihood (which we aim to minimize) is:\n",
        "\n",
        "$ -\\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right] $\n",
        "\n",
        "### Log-Loss Function:\n",
        "\n",
        "For machine learning purposes, particularly in training classification models like logistic regression, we define the log-loss function, which is the average of the negative log-likelihood across all observations. This is also called the cross-entropy loss in the context of binary classifications.\n",
        "\n",
        "$ \\text{Log-Loss} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right] $\n",
        "\n",
        "### Explanation of Log-Loss:\n",
        "\n",
        "- **Interpretation**: Log-loss penalizes incorrect classifications, but its penalty is not uniform; it depends on how confident the prediction was and whether it was correct or incorrect.\n",
        "- **Confident and Wrong**: A prediction that an event occurs with high probability, say \\( $p_i \\approx 1$ \\), when it actually does not occur ($y_i = 0$) results in a high loss because $\\log(1 - p_i)$ becomes a large negative number (log of a number close to zero is negative infinity).\n",
        "- **Confident and Right**: Conversely, if the prediction is highly confident and correct, the penalty is small (log of a number close to 1 is 0).\n",
        "- **Less Confident**: Predictions that are less confident (e.g., \\( $p_i$ \\) close to 0.5) have smaller penalties regardless of whether they are correct or not.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Thus, the log-loss function not only emphasizes accuracy but also the confidence of the predictions. It's an essential tool for developing robust binary classification models as it ensures that the models are not only accurate but also confident in their predictions, preventing overfitting to some extent. It encourages the model to predict probabilities as close as possible to 0 or 1, but only when it's almost certain, thereby making it integral for training stable and reliable models."
      ],
      "metadata": {
        "id": "g6xOwcd6ZlEd"
      }
    }
  ]
}