<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=na2xy4REgy2vjVAZtpZNcyWqADZi4Syx-AEk2KH_Q_i2hmvOwsMPqTIP4KVeiV_xBnIqpcFAcReH0Tw3JLCcnA);ul.lst-kix_f3tv83bt086d-1{list-style-type:none}ul.lst-kix_f3tv83bt086d-2{list-style-type:none}ul.lst-kix_f3tv83bt086d-0{list-style-type:none}ul.lst-kix_f3tv83bt086d-5{list-style-type:none}ul.lst-kix_f3tv83bt086d-6{list-style-type:none}ul.lst-kix_f3tv83bt086d-3{list-style-type:none}ul.lst-kix_f3tv83bt086d-4{list-style-type:none}ul.lst-kix_f3tv83bt086d-7{list-style-type:none}ul.lst-kix_f3tv83bt086d-8{list-style-type:none}.lst-kix_ac8fkp8ys4dl-6>li:before{content:"\0025cf   "}.lst-kix_ac8fkp8ys4dl-5>li:before{content:"\0025a0   "}.lst-kix_ac8fkp8ys4dl-8>li:before{content:"\0025a0   "}.lst-kix_ac8fkp8ys4dl-7>li:before{content:"\0025cb   "}ul.lst-kix_2txth8b9wzfu-1{list-style-type:none}ul.lst-kix_2txth8b9wzfu-0{list-style-type:none}ul.lst-kix_2txth8b9wzfu-5{list-style-type:none}ul.lst-kix_2txth8b9wzfu-4{list-style-type:none}ul.lst-kix_2txth8b9wzfu-3{list-style-type:none}.lst-kix_r8bjmlao7ho2-0>li:before{content:"\0025a0   "}ul.lst-kix_2txth8b9wzfu-2{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-4.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-4 0}ul.lst-kix_2txth8b9wzfu-8{list-style-type:none}ul.lst-kix_2txth8b9wzfu-7{list-style-type:none}ul.lst-kix_2txth8b9wzfu-6{list-style-type:none}.lst-kix_lwxg0pny7gv0-1>li:before{content:"\0025cb   "}.lst-kix_lwxg0pny7gv0-2>li:before{content:"\0025a0   "}.lst-kix_lwxg0pny7gv0-3>li:before{content:"\0025cf   "}.lst-kix_lwxg0pny7gv0-4>li:before{content:"\0025cb   "}.lst-kix_lxfzxemgo2e9-4>li:before{content:"\0025cb   "}.lst-kix_lwxg0pny7gv0-7>li:before{content:"\0025cb   "}.lst-kix_8w6oy4jdroww-3>li:before{content:"-  "}.lst-kix_8w6oy4jdroww-4>li:before{content:"-  "}.lst-kix_lwxg0pny7gv0-5>li:before{content:"\0025a0   "}.lst-kix_8w6oy4jdroww-2>li:before{content:"-  "}.lst-kix_8w6oy4jdroww-6>li:before{content:"-  "}.lst-kix_lxfzxemgo2e9-1>li:before{content:"\0025cb   "}.lst-kix_lxfzxemgo2e9-5>li:before{content:"\0025a0   "}.lst-kix_lwxg0pny7gv0-6>li:before{content:"\0025cf   "}.lst-kix_lxfzxemgo2e9-0>li:before{content:"\0025a0   "}.lst-kix_lxfzxemgo2e9-8>li:before{content:"\0025a0   "}.lst-kix_8w6oy4jdroww-0>li:before{content:"-  "}.lst-kix_8w6oy4jdroww-7>li:before{content:"-  "}.lst-kix_8w6oy4jdroww-8>li:before{content:"-  "}.lst-kix_lxfzxemgo2e9-6>li:before{content:"\0025cf   "}.lst-kix_8w6oy4jdroww-1>li:before{content:"-  "}.lst-kix_lxfzxemgo2e9-7>li:before{content:"\0025cb   "}.lst-kix_lwxg0pny7gv0-8>li:before{content:"\0025a0   "}.lst-kix_lxfzxemgo2e9-2>li:before{content:"\0025a0   "}ul.lst-kix_t402rmtng0om-0{list-style-type:none}ul.lst-kix_t402rmtng0om-1{list-style-type:none}.lst-kix_lxfzxemgo2e9-3>li:before{content:"\0025cf   "}ul.lst-kix_t402rmtng0om-2{list-style-type:none}.lst-kix_r4u3hmhmynip-4>li:before{content:"\0025cb   "}ul.lst-kix_t402rmtng0om-3{list-style-type:none}ul.lst-kix_txjetxy31k8u-4{list-style-type:none}ul.lst-kix_t402rmtng0om-4{list-style-type:none}ul.lst-kix_txjetxy31k8u-3{list-style-type:none}ul.lst-kix_t402rmtng0om-5{list-style-type:none}ul.lst-kix_txjetxy31k8u-2{list-style-type:none}ul.lst-kix_t402rmtng0om-6{list-style-type:none}ul.lst-kix_txjetxy31k8u-1{list-style-type:none}ul.lst-kix_t402rmtng0om-7{list-style-type:none}ul.lst-kix_txjetxy31k8u-0{list-style-type:none}.lst-kix_r4u3hmhmynip-5>li:before{content:"\0025a0   "}ul.lst-kix_t402rmtng0om-8{list-style-type:none}.lst-kix_ac8fkp8ys4dl-1>li:before{content:"\0025cb   "}.lst-kix_r4u3hmhmynip-1>li:before{content:"\0025cb   "}.lst-kix_ac8fkp8ys4dl-2>li:before{content:"\0025a0   "}.lst-kix_ac8fkp8ys4dl-3>li:before{content:"\0025cf   "}.lst-kix_r4u3hmhmynip-2>li:before{content:"\0025a0   "}ul.lst-kix_txjetxy31k8u-8{list-style-type:none}.lst-kix_ac8fkp8ys4dl-4>li:before{content:"\0025cb   "}ul.lst-kix_txjetxy31k8u-7{list-style-type:none}.lst-kix_r4u3hmhmynip-3>li:before{content:"\0025cf   "}ul.lst-kix_txjetxy31k8u-6{list-style-type:none}ul.lst-kix_txjetxy31k8u-5{list-style-type:none}.lst-kix_r4u3hmhmynip-0>li:before{content:"\0025cf   "}.lst-kix_kd2ieckbzqt2-3>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-3}.lst-kix_kd2ieckbzqt2-6>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-6}.lst-kix_ac8fkp8ys4dl-0>li:before{content:"\0025cf   "}.lst-kix_8w6oy4jdroww-5>li:before{content:"-  "}ol.lst-kix_kd2ieckbzqt2-3.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-3 0}ul.lst-kix_ac8fkp8ys4dl-0{list-style-type:none}ul.lst-kix_ac8fkp8ys4dl-1{list-style-type:none}.lst-kix_r4u3hmhmynip-8>li:before{content:"\0025a0   "}.lst-kix_r4u3hmhmynip-6>li:before{content:"\0025cf   "}ul.lst-kix_8w6oy4jdroww-7{list-style-type:none}ul.lst-kix_8w6oy4jdroww-8{list-style-type:none}.lst-kix_kd2ieckbzqt2-2>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-2}ol.lst-kix_kd2ieckbzqt2-8.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-8 0}.lst-kix_ins61pryps9h-8>li:before{content:"\0025a0   "}ul.lst-kix_8w6oy4jdroww-3{list-style-type:none}ul.lst-kix_8w6oy4jdroww-4{list-style-type:none}ul.lst-kix_8w6oy4jdroww-5{list-style-type:none}ul.lst-kix_8w6oy4jdroww-6{list-style-type:none}ul.lst-kix_8w6oy4jdroww-0{list-style-type:none}ul.lst-kix_8w6oy4jdroww-1{list-style-type:none}ul.lst-kix_8w6oy4jdroww-2{list-style-type:none}.lst-kix_ins61pryps9h-4>li:before{content:"\0025cb   "}ol.lst-kix_kd2ieckbzqt2-5.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-5 0}.lst-kix_kd2ieckbzqt2-7>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-7}.lst-kix_kd2ieckbzqt2-1>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-1}.lst-kix_txjetxy31k8u-2>li:before{content:"-  "}.lst-kix_ins61pryps9h-6>li:before{content:"\0025cf   "}.lst-kix_txjetxy31k8u-0>li:before{content:"-  "}.lst-kix_kd2ieckbzqt2-5>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-5,lower-roman) ". "}.lst-kix_lwxg0pny7gv0-0>li:before{content:"\0025cf   "}ul.lst-kix_r4u3hmhmynip-1{list-style-type:none}ul.lst-kix_r4u3hmhmynip-2{list-style-type:none}.lst-kix_txjetxy31k8u-6>li:before{content:"-  "}ul.lst-kix_r4u3hmhmynip-0{list-style-type:none}.lst-kix_kd2ieckbzqt2-7>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-7,lower-latin) ". "}.lst-kix_ins61pryps9h-0>li:before{content:"\0025cf   "}.lst-kix_kd2ieckbzqt2-8>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-8}ul.lst-kix_r4u3hmhmynip-7{list-style-type:none}ul.lst-kix_r4u3hmhmynip-8{list-style-type:none}.lst-kix_ins61pryps9h-2>li:before{content:"\0025a0   "}ul.lst-kix_r4u3hmhmynip-5{list-style-type:none}ul.lst-kix_r4u3hmhmynip-6{list-style-type:none}.lst-kix_txjetxy31k8u-4>li:before{content:"-  "}.lst-kix_r8bjmlao7ho2-2>li:before{content:"\0025a0   "}ul.lst-kix_r4u3hmhmynip-3{list-style-type:none}ul.lst-kix_r4u3hmhmynip-4{list-style-type:none}.lst-kix_r8bjmlao7ho2-4>li:before{content:"\0025cb   "}.lst-kix_r8bjmlao7ho2-8>li:before{content:"\0025a0   "}.lst-kix_r8bjmlao7ho2-6>li:before{content:"\0025cf   "}ul.lst-kix_ac8fkp8ys4dl-2{list-style-type:none}ul.lst-kix_ac8fkp8ys4dl-3{list-style-type:none}ul.lst-kix_ac8fkp8ys4dl-4{list-style-type:none}.lst-kix_t402rmtng0om-5>li:before{content:"-  "}.lst-kix_t402rmtng0om-7>li:before{content:"-  "}ul.lst-kix_ac8fkp8ys4dl-5{list-style-type:none}ul.lst-kix_ac8fkp8ys4dl-6{list-style-type:none}ul.lst-kix_ac8fkp8ys4dl-7{list-style-type:none}.lst-kix_kd2ieckbzqt2-3>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-3,decimal) ". "}ul.lst-kix_ac8fkp8ys4dl-8{list-style-type:none}.lst-kix_kd2ieckbzqt2-1>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-1,lower-latin) ". "}ol.lst-kix_kd2ieckbzqt2-6.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-6 0}.lst-kix_c11g73hedk19-3>li:before{content:"-  "}.lst-kix_c11g73hedk19-2>li:before{content:"-  "}.lst-kix_c11g73hedk19-4>li:before{content:"-  "}.lst-kix_c11g73hedk19-1>li:before{content:"-  "}.lst-kix_c11g73hedk19-5>li:before{content:"-  "}.lst-kix_t402rmtng0om-2>li:before{content:"-  "}.lst-kix_c11g73hedk19-7>li:before{content:"-  "}.lst-kix_c11g73hedk19-0>li:before{content:"-  "}.lst-kix_c11g73hedk19-6>li:before{content:"-  "}.lst-kix_c11g73hedk19-8>li:before{content:"-  "}.lst-kix_t402rmtng0om-3>li:before{content:"-  "}.lst-kix_t402rmtng0om-4>li:before{content:"-  "}ul.lst-kix_ins61pryps9h-6{list-style-type:none}ul.lst-kix_ins61pryps9h-5{list-style-type:none}ul.lst-kix_ins61pryps9h-8{list-style-type:none}ul.lst-kix_ins61pryps9h-7{list-style-type:none}ul.lst-kix_ins61pryps9h-2{list-style-type:none}ul.lst-kix_ins61pryps9h-1{list-style-type:none}ul.lst-kix_ins61pryps9h-4{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-7.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-7 0}ul.lst-kix_ins61pryps9h-3{list-style-type:none}.lst-kix_2txth8b9wzfu-0>li:before{content:"\0025cf   "}.lst-kix_t402rmtng0om-1>li:before{content:"-  "}.lst-kix_t402rmtng0om-0>li:before{content:"-  "}.lst-kix_2txth8b9wzfu-4>li:before{content:"\0025cb   "}.lst-kix_2txth8b9wzfu-5>li:before{content:"\0025a0   "}.lst-kix_f3tv83bt086d-1>li:before{content:"\0025cb   "}.lst-kix_f3tv83bt086d-0>li:before{content:"\0025cf   "}.lst-kix_f3tv83bt086d-3>li:before{content:"\0025cf   "}.lst-kix_2txth8b9wzfu-1>li:before{content:"\0025cb   "}ul.lst-kix_ins61pryps9h-0{list-style-type:none}.lst-kix_2txth8b9wzfu-2>li:before{content:"\0025a0   "}.lst-kix_2txth8b9wzfu-3>li:before{content:"\0025cf   "}.lst-kix_txjetxy31k8u-8>li:before{content:"-  "}.lst-kix_f3tv83bt086d-2>li:before{content:"\0025a0   "}.lst-kix_f3tv83bt086d-7>li:before{content:"\0025cb   "}.lst-kix_f3tv83bt086d-5>li:before{content:"\0025a0   "}.lst-kix_f3tv83bt086d-4>li:before{content:"\0025cb   "}.lst-kix_f3tv83bt086d-8>li:before{content:"\0025a0   "}.lst-kix_2txth8b9wzfu-8>li:before{content:"\0025a0   "}ol.lst-kix_kd2ieckbzqt2-1.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-1 0}.lst-kix_2txth8b9wzfu-6>li:before{content:"\0025cf   "}.lst-kix_2txth8b9wzfu-7>li:before{content:"\0025cb   "}.lst-kix_f3tv83bt086d-6>li:before{content:"\0025cf   "}ul.lst-kix_r8bjmlao7ho2-3{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-2{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-5{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-4{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-7{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-6{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-8{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-2.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-2 0}ul.lst-kix_lxfzxemgo2e9-0{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-2{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-1{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-1{list-style-type:none}ul.lst-kix_r8bjmlao7ho2-0{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-4{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-3{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-6{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-5{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-8{list-style-type:none}ul.lst-kix_lxfzxemgo2e9-7{list-style-type:none}.lst-kix_kd2ieckbzqt2-0>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-0}.lst-kix_kd2ieckbzqt2-4>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-4}ul.lst-kix_c11g73hedk19-7{list-style-type:none}ul.lst-kix_c11g73hedk19-8{list-style-type:none}ul.lst-kix_c11g73hedk19-5{list-style-type:none}ul.lst-kix_c11g73hedk19-6{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-0.start{counter-reset:lst-ctn-kix_kd2ieckbzqt2-0 0}ol.lst-kix_kd2ieckbzqt2-7{list-style-type:none}.lst-kix_kd2ieckbzqt2-5>li{counter-increment:lst-ctn-kix_kd2ieckbzqt2-5}ol.lst-kix_kd2ieckbzqt2-8{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-5{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-6{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-3{list-style-type:none}ul.lst-kix_c11g73hedk19-3{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-4{list-style-type:none}.lst-kix_r4u3hmhmynip-7>li:before{content:"\0025cb   "}ul.lst-kix_c11g73hedk19-4{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-1{list-style-type:none}ul.lst-kix_c11g73hedk19-1{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-2{list-style-type:none}ul.lst-kix_c11g73hedk19-2{list-style-type:none}ol.lst-kix_kd2ieckbzqt2-0{list-style-type:none}ul.lst-kix_c11g73hedk19-0{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-0{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-1{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-2{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-3{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-4{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-5{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-6{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-7{list-style-type:none}ul.lst-kix_lwxg0pny7gv0-8{list-style-type:none}.lst-kix_ins61pryps9h-3>li:before{content:"\0025cf   "}.lst-kix_ins61pryps9h-5>li:before{content:"\0025a0   "}.lst-kix_txjetxy31k8u-1>li:before{content:"-  "}.lst-kix_ins61pryps9h-7>li:before{content:"\0025cb   "}.lst-kix_txjetxy31k8u-5>li:before{content:"-  "}.lst-kix_kd2ieckbzqt2-6>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-6,decimal) ". "}.lst-kix_txjetxy31k8u-3>li:before{content:"-  "}.lst-kix_txjetxy31k8u-7>li:before{content:"-  "}.lst-kix_r8bjmlao7ho2-1>li:before{content:"\0025cb   "}.lst-kix_kd2ieckbzqt2-8>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-8,lower-roman) ". "}.lst-kix_ins61pryps9h-1>li:before{content:"\0025cb   "}.lst-kix_r8bjmlao7ho2-3>li:before{content:"\0025cf   "}.lst-kix_r8bjmlao7ho2-5>li:before{content:"\0025a0   "}.lst-kix_r8bjmlao7ho2-7>li:before{content:"\0025cb   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_kd2ieckbzqt2-4>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-4,lower-latin) ". "}.lst-kix_kd2ieckbzqt2-2>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-2,lower-roman) ". "}.lst-kix_t402rmtng0om-6>li:before{content:"-  "}.lst-kix_kd2ieckbzqt2-0>li:before{content:"" counter(lst-ctn-kix_kd2ieckbzqt2-0,decimal) ". "}.lst-kix_t402rmtng0om-8>li:before{content:"-  "}ol{margin:0;padding:0}table td,table th{padding:0}.c11{border-right-style:solid;padding-top:0pt;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.5;border-left-width:0pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;orphans:2;widows:2;text-align:left;padding-right:0pt}.c25{border-right-style:solid;padding-top:0pt;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.5;border-top-style:solid;background-color:#ffffff;margin-left:68pt;border-bottom-width:0pt;border-bottom-style:solid;orphans:2;widows:2;text-align:left;padding-right:0pt}.c23{border-right-style:solid;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;border-left-width:0pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;padding-right:0pt}.c31{border-right-style:solid;border-top-width:0pt;border-right-width:0pt;page-break-after:avoid;border-top-style:solid;background-color:#ffffff;border-bottom-width:0pt;border-bottom-style:solid;padding-right:0pt}.c33{border-right-style:solid;border-top-width:0pt;border-right-width:0pt;border-top-style:solid;background-color:#ffffff;border-bottom-width:0pt;border-bottom-style:solid;padding-right:0pt}.c14{background-color:#ffffff;color:#212529;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Roboto";font-style:normal}.c21{background-color:#ffffff;color:#212529;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Roboto";font-style:normal}.c16{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c12{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left;height:16pt}.c27{background-color:#ffffff;padding-top:0pt;padding-bottom:11pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c2{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c43{color:#05192d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13.5pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c9{color:#202124;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Roboto";font-style:normal}.c42{color:#ececec;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Roboto";font-style:normal}.c10{color:#05192d;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c38{background-color:#ffffff;padding-top:11pt;padding-bottom:11pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c26{padding-top:0pt;padding-bottom:15pt;line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{padding-top:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c30{background-color:#ffffff;font-size:12pt;font-family:"Roboto";color:#212529;font-weight:400}.c41{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c24{background-color:#ffffff;font-size:12pt;font-family:"Roboto";color:#212529;font-weight:700}.c22{background-color:#ffffff;font-size:10.5pt;font-family:"Consolas";color:#2878a2;font-weight:700}.c5{background-color:#ecf0f3;font-size:10.5pt;font-family:"Consolas";color:#222222;font-weight:400}.c19{font-size:12pt;font-family:"Roboto";color:#202124;font-weight:700}.c17{font-size:12pt;font-family:"Roboto";color:#202124;font-weight:400}.c18{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c20{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#0766d1;text-decoration:underline}.c35{background-color:#ecf0f3;font-size:10.5pt;font-family:"Consolas";color:#222222}.c39{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c28{background-color:#ffffff;font-family:"Calibri";font-weight:400}.c36{background-color:#ffffff;font-size:12pt;color:#16191f}.c37{text-decoration:none;vertical-align:baseline;font-style:italic}.c8{color:inherit;text-decoration:inherit}.c29{padding:0;margin:0}.c44{background-color:#ffffff;font-size:10.5pt}.c40{background-color:#ffffff;font-size:12pt}.c6{margin-left:36pt;padding-left:0pt}.c32{font-weight:700}.c4{height:11pt}.c34{height:20pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c39 doc-content"><h1 class="c2" id="h.d63ld86t34ng"><span class="c3">Machine Learning Breadth Interview Preparation Study Guide</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Name: John Hodge</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Date: 05/13/24</span></p><h1 class="c2" id="h.xlvvrch5cmqq"><span class="c3">Precision vs. Recall</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 368.59px; height: 670.40px;"><img alt="" src="images/image25.png" style="width: 368.59px; height: 670.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.00px; height: 261.00px;"><img alt="" src="images/image16.png" style="width: 507.00px; height: 261.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 243.00px; height: 202.00px;"><img alt="" src="images/image24.png" style="width: 243.00px; height: 202.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Precision and recall are two fundamental metrics used in the evaluation of classification models, especially when dealing with imbalanced datasets. They help provide insights into how well a model is performing with respect to both positive class prediction and the relevance of the results it provides. Here&#39;s a breakdown of each:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">1. **Precision**: This metric indicates the accuracy of positive predictions. It is defined as the ratio of true positive results to the total number of examples predicted as positive. In simpler terms, precision answers the question: &quot;Of all the examples the model labeled as positive, how many actually belong to the positive class?&quot; Precision is crucial when the cost of a false positive is high.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">&nbsp; &nbsp;**Formula**: </span></p><p class="c1"><span class="c0">&nbsp; &nbsp;\[</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;\text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP) + False Positives (FP)}}</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;\]</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **Recall** (also known as Sensitivity or True Positive Rate): This metric measures the model&rsquo;s ability to detect positive instances among the actual positive instances available during testing. It is defined as the ratio of true positive results to the actual number of examples that are positive. Recall addresses the question: &quot;Of all the actual positives, how many did the model successfully identify?&quot; This metric is critical when the cost of missing a positive (false negative) is severe.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">&nbsp; &nbsp;**Formula**: </span></p><p class="c1"><span class="c0">&nbsp; &nbsp;\[</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;\text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP) + False Negatives (FN)}}</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;\]</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">In practice, precision and recall are often in a trade-off relationship. Increasing precision typically reduces recall and vice versa, depending on the threshold set for classifying a prediction as positive. This is why another metric, called the F1-score, is often used to find a balance between precision and recall, providing a single measure of a model&rsquo;s accuracy that factors in both metrics.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Would you like to delve deeper into how these metrics are applied or maybe explore some examples?</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&amp;sa=D&amp;source=editors&amp;ust=1715648942473371&amp;usg=AOvVaw1LNs4bzQhYKAvKlcbwZXjN">https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall</a></span></p><h1 class="c2 c34" id="h.iewabkabsi30"><span class="c3"></span></h1><h1 class="c2" id="h.r93g0si9p337"><span class="c3">ROC-AUC</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&amp;sa=D&amp;source=editors&amp;ust=1715648942473680&amp;usg=AOvVaw3iY5DgKCWHaDVpd4hSaMro">https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c23 c1"><span class="c17">An </span><span class="c19">ROC curve</span><span class="c17">&nbsp;(</span><span class="c19">receiver operating characteristic curve</span><span class="c9">) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:</span></p><ul class="c29 lst-kix_ins61pryps9h-0 start"><li class="c33 c1 c6 li-bullet-0"><span class="c9">True Positive Rate</span></li><li class="c33 c1 c6 li-bullet-0"><span class="c9">False Positive Rate</span></li></ul><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c23 c1"><span class="c9">AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.</span></p><p class="c23 c1"><span class="c9">AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.</span></p><p class="c23 c1"><span class="c9">AUC is desirable for the following two reasons:</span></p><ul class="c29 lst-kix_2txth8b9wzfu-0 start"><li class="c33 c1 c6 li-bullet-0"><span class="c17">AUC is </span><span class="c19">scale-invariant</span><span class="c9">. It measures how well predictions are ranked, rather than their absolute values.</span></li><li class="c1 c6 c33 li-bullet-0"><span class="c17">AUC is </span><span class="c19">classification-threshold-invariant</span><span class="c9">. It measures the quality of the model&#39;s predictions irrespective of what classification threshold is chosen.</span></li></ul><p class="c1 c23"><span class="c9">However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:</span></p><ul class="c29 lst-kix_ac8fkp8ys4dl-0 start"><li class="c33 c1 c6 li-bullet-0"><span class="c19">Scale invariance is not always desirable.</span><span class="c9">&nbsp;For example, sometimes we really do need well calibrated probability outputs, and AUC won&rsquo;t tell us about that.</span></li><li class="c33 c1 c6 li-bullet-0"><span class="c19">Classification-threshold invariance is not always desirable.</span><span class="c9">&nbsp;In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn&#39;t a useful metric for this type of optimization.</span></li></ul><p class="c33 c1 c4"><span class="c9"></span></p><h1 class="c1 c31" id="h.q6viwlkvh9bs"><span class="c3">Adam Optimizer vs. SGD</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The Adam optimizer and stochastic gradient descent (SGD) are both algorithms used for optimizing the training process of neural networks, but they have key differences in how they approach the optimization problem, which often makes Adam more effective in practice.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Stochastic Gradient Descent (SGD)</span></p><p class="c1"><span class="c0">SGD is one of the simplest and oldest optimization techniques used for training neural networks. It updates the model&#39;s weights by taking a step proportional to the negative of the gradient of the objective function with respect to the weights. The basic formula for the weight update is:</span></p><p class="c1"><span class="c0">\[ w = w - \eta \nabla_w J(w) \]</span></p><p class="c1"><span class="c0">where \( w \) represents the weights, \( \eta \) is the learning rate, and \( \nabla_w J(w) \) is the gradient of the loss function \( J \) with respect to the weights \( w \).</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">SGD updates weights using the same learning rate for all parameters and does not take into account the history of gradients. This can lead to several issues such as slow convergence, getting stuck in local minima, or issues with the chosen learning rate not being adaptive.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Adam Optimizer</span></p><p class="c1"><span class="c0">Adam, short for &quot;Adaptive Moment Estimation,&quot; builds on SGD by incorporating both the averages of the first moments (the mean) and the second moments (the uncentered variance) of the gradients. This allows Adam to adjust the learning rate for each weight individually by:</span></p><p class="c1"><span class="c0">1. Calculating an exponentially weighted average of past gradients (momentum).</span></p><p class="c1"><span class="c0">2. Calculating an exponentially weighted average of past squared gradients (this adjusts the learning rate based on the variance of the gradients).</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The update rules for Adam are given by:</span></p><p class="c1"><span class="c0">\[ m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_w J(w) \]</span></p><p class="c1"><span class="c0">\[ v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_w J(w))^2 \]</span></p><p class="c1"><span class="c0">\[ \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \]</span></p><p class="c1"><span class="c0">\[ \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \]</span></p><p class="c1"><span class="c0">\[ w = w - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} \]</span></p><p class="c1"><span class="c0">where \( \beta_1 \) and \( \beta_2 \) are decay rates for the moving averages, generally set to values like 0.9 and 0.999, respectively, and \( \epsilon \) is a small constant (e.g., \( 10^{-8} \)) to prevent division by zero.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Why Adam is Often Used</span></p><p class="c1"><span class="c0">Adam is favored in many neural network training scenarios because:</span></p><p class="c1"><span class="c0">1. **Adaptive Learning Rates**: By adjusting the learning rate for each parameter, Adam often leads to faster convergence.</span></p><p class="c1"><span class="c0">2. **Built-in Momentum**: By considering the first moment of the gradients, it incorporates a momentum mechanism that helps to accelerate the gradients vectors in the right directions, thus leading to faster converging.</span></p><p class="c1"><span class="c0">3. **Handling Sparse Gradients**: Adam works well with sparse gradients (common in fields like natural language processing), as it inherently performs smaller updates for infrequent parameters.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Due to these advantages, Adam is often the go-to optimizer for training deep neural networks, especially when dealing with problems that involve complex data structures and large datasets. However, it&#39;s important to note that Adam can sometimes lead to poor generalization compared to SGD with carefully tuned learning rates and momentum, especially in settings where minimal overfitting is crucial.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://chat.openai.com/share/d80bc04a-a35d-4ed6-9dfe-d2537752052c&amp;sa=D&amp;source=editors&amp;ust=1715648942476369&amp;usg=AOvVaw1C3woLpbLZEGom7HnlNLaf">https://chat.openai.com/share/d80bc04a-a35d-4ed6-9dfe-d2537752052c</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.3kbs2w1uvk18"><span class="c3">Random Forests </span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 592.00px; height: 444.00px;"><img alt="" src="images/image14.png" style="width: 592.00px; height: 444.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c28">A form of ensemble classifier. </span><span class="c28">A random forest is a machine learning algorithm that combines the results of multiple decision trees to produce a single result. It&#39;s a popular ensemble learning method used for classification, regression, and other tasks.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Random Forest is a versatile machine learning algorithm capable of performing both regression and classification tasks. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model. Here&#39;s a breakdown of how Random Forest works, particularly focusing on classification:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Basic Concept</span></p><p class="c1"><span class="c0">Random Forest builds multiple decision trees and merges them together to get a more accurate and stable prediction. One big advantage of Random Forest over a single decision tree is that it reduces the risk of overfitting, which is a common problem with decision trees.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### How It Works</span></p><p class="c1"><span class="c0">1. **Bootstrap Aggregating (Bagging):** Random Forest starts by performing bootstrap aggregating, or bagging, where multiple subsets of the original dataset are created with replacement. This means each subset can have duplicate records, and some records may be left out of some subsets.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **Building Decision Trees:** A decision tree is built for each of these subsets. The tree construction is done by randomly selecting a subset of features at each node to determine the split. The number of features that can be selected is less than the total number of features and is a key parameter that can be adjusted. This randomness in feature selection contributes to making the trees diverse and robust against overfitting.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">3. **Tree Growth:** Each tree in a Random Forest grows to its maximum length and is not pruned (though this can be adjusted depending on the implementation). This full growth usually involves creating very deep trees that are highly complex and can capture a lot of detailed data points.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">4. **Prediction:** For a classification problem, each tree in the forest outputs a class prediction, and the final output prediction is the class that receives the majority of the votes from the trees. In essence, Random Forest applies a &quot;majority rules&quot; principle where each individual tree votes for a class, and the class with the most votes becomes the model&rsquo;s prediction.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Key Parameters</span></p><p class="c1"><span class="c0">- **Number of Trees:** The number of trees in the forest. More trees increase the prediction robustness but also the computational complexity.</span></p><p class="c1"><span class="c0">- **Max Features:** The number of features to consider when looking for the best split at each node. Common values are the square root of the total features or about one-third of the total.</span></p><p class="c1"><span class="c0">- **Max Depth:** The maximum depth of each tree. Deeper trees can learn more detailed data specifics but can lead to overfitting.</span></p><p class="c1"><span class="c0">- **Min Samples Split:** The minimum number of samples a node must have before it can be split.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Advantages</span></p><p class="c1"><span class="c0">- **Robustness:** Due to averaging multiple trees, it is less sensitive to outliers and noise.</span></p><p class="c1"><span class="c0">- **Less Overfitting:** Randomness in feature selection and bootstrap aggregating help it to generalize better.</span></p><p class="c1"><span class="c0">- **Feature Importance:** It can handle large data sets with higher dimensionality well and can provide estimates of what variables are important in the underlying data being modeled.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Disadvantages</span></p><p class="c1"><span class="c0">- **Complexity:** More computationally intensive than other algorithms, especially with a large number of trees.</span></p><p class="c1"><span class="c0">- **Interpretability:** Less interpretable than individual decision trees due to its complex ensemble structure.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Random Forest is widely used in various fields, from banking for credit scoring to e-commerce for recommendation engines, due to its versatility, ease of use, and robust performance on many tasks.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html&amp;sa=D&amp;source=editors&amp;ust=1715648942478212&amp;usg=AOvVaw0ErcBWFCQhlUHJMkuyRvRh">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://www.ibm.com/topics/random-forest%23:~:text%3DRandom%2520forest%2520is%2520a%2520commonly,both%2520classification%2520and%2520regression%2520problems&amp;sa=D&amp;source=editors&amp;ust=1715648942478403&amp;usg=AOvVaw3srZqaooNGdUj9ks3ylfah">https://www.ibm.com/topics/random-forest#:~:text=Random%20forest%20is%20a%20commonly,both%20classification%20and%20regression%20problems</a></span><span class="c0">.</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.9ymqbtbc7x3p"><span class="c3">XGBoost</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18 c40"><a class="c8" href="https://www.google.com/url?q=https://github.com/dmlc/xgboost&amp;sa=D&amp;source=editors&amp;ust=1715648942478727&amp;usg=AOvVaw26GY28WZlARbw71cADDzWH">XGBoost</a></span><span class="c36">&nbsp;is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 961.00px; height: 651.00px;"><img alt="" src="images/image20.png" style="width: 961.00px; height: 651.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12.pdf%23page%3D380&amp;sa=D&amp;source=editors&amp;ust=1715648942479096&amp;usg=AOvVaw2Llm2ax__lKIUUEbwaJMoI">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12.pdf#page=380</a></span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.37xylna402s2"><span class="c7">Gradient Boosting</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Gradient boosting is a machine learning technique used for both regression and classification problems. It builds an ensemble of weak prediction models, typically decision trees, in a stage-wise fashion. Here&#39;s a detailed breakdown of how gradient boosting works:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### 1. **Base Model Setup:**</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;Gradient boosting starts with a simple model. Often, this initial model is just a single prediction that minimizes the loss function (like the mean of the target values for regression).</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### 2. **Learning from Mistakes:**</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;The core idea is to sequentially add new models (usually decision trees) that effectively correct the errors made by previous models. The method focuses on improving the accuracy in areas where the previous models performed poorly.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### 3. **Gradient Descent on the Loss Function:**</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;Instead of updating the model parameters directly, gradient boosting updates the model by fitting new models to the residual errors made by previous predictions. This is analogous to performing a step of gradient descent on the loss function that measures the difference between the predicted and actual values. The &quot;gradient&quot; in gradient boosting refers to the gradient of the loss function, which guides how the model&#39;s predictions should be corrected.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### 4. **Additive Model Construction:**</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;Each new tree added is fitted on the residual errors of the ensemble thus far. The predictions from all the trees are then summed together to make the final prediction. This additive model construction continues until a specified number of trees are added, or if adding new trees ceases to improve the accuracy significantly.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### 5. **Regularization Techniques:**</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;To prevent overfitting, gradient boosting incorporates several regularization techniques such as limiting the number of trees, tree depth, learning rate (also known as shrinkage or step-size), and employing subsampling techniques like Stochastic Gradient Boosting.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### 6. **Output Decision:**</span></p><p class="c1"><span class="c0">&nbsp; &nbsp;For classification, the ensemble of trees votes or averages to predict the class. For regression, the outputs of all trees are summed up to predict the continuous value.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Applications and Strengths:</span></p><p class="c1"><span class="c0">- **Robustness to Overfitting:** Especially with small datasets and when using regularization techniques.</span></p><p class="c1"><span class="c0">- **Handling of Heterogeneous Features:** Effective with datasets where features are of different types (e.g., binary, categorical, continuous).</span></p><p class="c1"><span class="c0">- **Feature Importance:** Can be used to rank features in terms of their usefulness.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Gradient boosting is powerful but also computationally demanding compared to models like random forests, especially as the number of trees increases. Libraries like XGBoost, LightGBM, and CatBoost have optimized implementations of gradient boosting that are widely used in machine learning competitions and real-world applications due to their speed and performance.</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.aestbsk2q357"><span class="c3">Bias vs. Variance Trade-off</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image8.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The trade-off between bias and variance is a fundamental concept in machine learning and statistics, related to the problem of overfitting and underfitting. Here&#39;s a closer look at each component and how they interact:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">1. **Bias**: This refers to the error introduced by approximating a real-world problem, which may be complex, by a much simpler model. Bias measures how far off on average these model predictions are from the actual value. A high-bias model is overly simplistic &ndash; it doesn&#39;t learn well from the training data, missing relevant relations between features and target outputs (underfitting). Models with high bias are usually simple (linear models) and do not capture the complex patterns in data.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **Variance**: Variance measures how much the predictions for a given point vary between different realizations of the model. High variance suggests that the model is extremely sensitive to small fluctuations in the training set. High-variance models often follow the training data very closely, including the noise in the data, leading to overfitting. These models perform well on training data but poorly on any unseen test data.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The trade-off:</span></p><p class="c1"><span class="c0">- A model with high bias and low variance will generalize better but at the cost of failing to capture important nuances (underfits).</span></p><p class="c1"><span class="c0">- A model with low bias and high variance captures the training data very well but does not generalize to new, unseen data (overfits).</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">In practice, you want to find a balance between bias and variance that minimizes the total error. This is often managed by:</span></p><p class="c1"><span class="c0">- Choosing the right model complexity that is appropriate for the size and variety of the data.</span></p><p class="c1"><span class="c0">- Employing techniques like cross-validation to validate model performance on unseen data.</span></p><p class="c1"><span class="c0">- Using regularization techniques (like LASSO, Ridge) that can penalize overly complex models, thus reducing variance without increasing bias too much.</span></p><p class="c1"><span class="c0">- Adjusting model hyperparameters to find the optimal balance.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Finding this balance helps in building models that are both accurate and robust enough to perform well on new, unseen data.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.eb9m6yq3rdxx"><span class="c3">Class imbalance</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image23.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://scikit-learn.org/stable/modules/model_evaluation.html%23classification-report&amp;sa=D&amp;source=editors&amp;ust=1715648942481992&amp;usg=AOvVaw2vGZh1sqmAEG83U-9jO4Xm">https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 957.00px; height: 591.00px;"><img alt="" src="images/image11.png" style="width: 957.00px; height: 591.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c24">average</span><span class="c24 c37">{&lsquo;micro&rsquo;, &lsquo;macro&rsquo;, &lsquo;samples&rsquo;, &lsquo;weighted&rsquo;, &lsquo;binary&rsquo;} or None, default=&rsquo;binary&rsquo;</span></p><p class="c15"><span class="c30">This parameter is required for multiclass/multilabel targets. If </span><span class="c5">None</span><span class="c21">, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</span></p><p class="c1"><span class="c35 c32">&#39;binary&#39;</span><span class="c14">:</span></p><p class="c15"><span class="c30">Only report results for the class specified by </span><span class="c5">pos_label</span><span class="c30">. This is applicable only if targets (</span><span class="c5">y_{true,pred}</span><span class="c21">) are binary.</span></p><p class="c1"><span class="c35 c32">&#39;micro&#39;</span><span class="c14">:</span></p><p class="c15"><span class="c21">Calculate metrics globally by counting the total true positives, false negatives and false positives.</span></p><p class="c1"><span class="c32 c35">&#39;macro&#39;</span><span class="c14">:</span></p><p class="c15"><span class="c21">Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</span></p><p class="c1"><span class="c35 c32">&#39;weighted&#39;</span><span class="c14">:</span></p><p class="c15"><span class="c21">Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters &lsquo;macro&rsquo; to account for label imbalance; it can result in an F-score that is not between precision and recall.</span></p><p class="c1"><span class="c35 c32">&#39;samples&#39;</span><span class="c14">:</span></p><p class="c15"><span class="c30">Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from </span><span class="c22"><a class="c8" href="https://www.google.com/url?q=https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html%23sklearn.metrics.accuracy_score&amp;sa=D&amp;source=editors&amp;ust=1715648942483091&amp;usg=AOvVaw2rpuad_lxwzZLzy8_7Wmpe">accuracy_score</a></span><span class="c21">).</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.rq7cgmtgg3rn"><span class="c3">Bayes Thereom / Bayesian Inference </span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image7.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.2d97vltvpgw8"><span class="c3">Principal Component Analysis and Dimensionality Reduction</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image6.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://scikit-learn.org/stable/modules/decomposition.html%23decompositions&amp;sa=D&amp;source=editors&amp;ust=1715648942483657&amp;usg=AOvVaw1FI-vMYb8yvAxdCXArACwZ">https://scikit-learn.org/stable/modules/decomposition.html#decompositions</a></span></p><h1 class="c2" id="h.uwn1cvyjln9d"><span class="c3">Classifiers (Classification Problems)</span></h1><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://scikit-learn.org/stable/supervised_learning.html%23supervised-learning&amp;sa=D&amp;source=editors&amp;ust=1715648942483892&amp;usg=AOvVaw3fklJ4-wDbu3iNQnOEIIio">https://scikit-learn.org/stable/supervised_learning.html#supervised-learning</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 2048.00px; height: 683.00px;"><img alt="" src="images/image9.png" style="width: 2048.00px; height: 683.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.dm0yhmml01ky"><span class="c7">Regression Problems</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image3.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.hfguag4fqp15"><span class="c3">Starbucks project</span></h1><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://github.com/jman4162/udacity-mle-starbucks-capstone-project&amp;sa=D&amp;source=editors&amp;ust=1715648942484701&amp;usg=AOvVaw0YhCTOUdYeXfipLbRYEkJx">https://github.com/jman4162/udacity-mle-starbucks-capstone-project</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Algorithms and Techniques We perform customer segmentation using the k-means clustering algorithm and use principal component analysis (PCA) for dimensionality reduction and visualization. For the classification problems, we evaluate the logistic regression (LogReg), support vector machine (SVM), k-nearest neighbor (kNN), XGBoost, and deep neural network (DNN) algorithms. We evaluate the linear stochastic gradient descent (SGD) model, support vector regression (SVR), kNN, XGBoost, and DNN models for regression problems. XGBoost is an open-source and efficient implementation of the gradient boosted tree algorithm. Gradient boosting is a supervised learning algorithm used in one of the course lessons to predict housing price data. I used DNN models extensively using PyTorch in the Deep Learning Nanodegree program.</span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.yuerxave9n2t"><span class="c7">Project Overview </span></h2><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://chat.openai.com/share/6132b813-148d-4ff1-97ab-806a6599aefb&amp;sa=D&amp;source=editors&amp;ust=1715648942485059&amp;usg=AOvVaw2d1AGdmxpoEZvl2kMuyfCK">https://chat.openai.com/share/6132b813-148d-4ff1-97ab-806a6599aefb</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">In your capstone project for the Udacity Machine Learning Engineer Nanodegree, you focused on leveraging the Starbucks app data to tackle two main business challenges:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">1. **Predicting Customer Engagement with Promotions**: You developed predictive models to ascertain whether a customer would view and complete a promotional offer based on their demographic information and the characteristics of the offer. This involved a classification approach, using metrics like precision, accuracy, recall, and the area under the ROC curve to evaluate model performance.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **Forecasting Revenue from Promotions**: You created a regression model to estimate the revenue generated from a completed offer after accounting for the cost of rewards. This model aimed to help Starbucks maximize ROI by predicting the financial outcome of promotional strategies. Key metrics for this model included RMSE, R2 score, and explained variance.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">**Key Techniques and Technologies**:</span></p><p class="c1"><span class="c0">- You integrated data from multiple sources (portfolio, profile, and transaction datasets) and applied exploratory data analysis, data cleaning, and feature engineering.</span></p><p class="c1"><span class="c0">- You used various machine learning algorithms including logistic regression, support vector machines, k-nearest neighbors, XGBoost, and deep neural networks implemented in Python, utilizing libraries like Scikit-learn and PyTorch.</span></p><p class="c1"><span class="c0">- You evaluated these models against benchmark models (logistic regression for classification and linear SGD for regression) and optimized them through hyperparameter tuning.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">**Outcomes and Business Impact**:</span></p><p class="c1"><span class="c0">- Your models successfully predicted customer engagement with promotional offers and the associated revenue impacts, thereby enabling personalized marketing strategies.</span></p><p class="c1"><span class="c0">- The analysis provided insights into customer behavior, helping to tailor promotions effectively and improving the strategic deployment of marketing resources.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">**Conclusion**:</span></p><p class="c1"><span class="c0">- This project not only demonstrated the application of machine learning to real-world business problems but also emphasized the value of data-driven decision-making in marketing strategies. You showcased how predictive analytics could enhance customer engagement and optimize promotional expenditures.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">You can present these points in your interview to highlight your proficiency in handling complex datasets, applying advanced machine learning techniques, and driving business value through analytics.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.cwjce4ph78fd"><span class="c7">Results overview </span></h2><ul class="c29 lst-kix_t402rmtng0om-0 start"><li class="c1 c6 li-bullet-0"><span class="c0">XGBoost performed best for the classification problem</span></li><li class="c1 c6 li-bullet-0"><span class="c0">DNN performed best for regression problem (revenue prediction)</span></li></ul><h2 class="c13" id="h.7dfmk2jucps5"><span class="c7">Project overview from the report</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The object of this capstone project is to answer two business challenges using the Starbucks dataset: </span></p><p class="c1 c4"><span class="c0"></span></p><ul class="c29 lst-kix_txjetxy31k8u-0 start"><li class="c1 c6 li-bullet-0"><span class="c0">I. Can we use data to build a predictive model to determine whether a customer will view and complete a promotional offer based on the offer characteristics and customer demographic information? </span></li><li class="c1 c6 li-bullet-0"><span class="c0">II. Can we use offer characteristics and customer demographic data to predict how much revenue a completed offer will generate after subtracting reward costs? </span></li></ul><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">I analyze five different machine learning (ML) predictive model solutions and compare their performance for three case studies. These predictive models allow the business to personalize the characteristics of each promotional offer based on each customer&rsquo;s demographic profile to maximize the probability that the customer views and completes a promotional offer. Additionally, the regression model predicts how much revenue a completed promotional offer generates after subtracting the reward cost to help the business maximize return on investment (ROI) per customer.</span></p><h2 class="c13" id="h.mftl3h9of1sb"><span class="c7">Conclusion from report</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Overall, this project was surprisingly challenging due to the structure of the transcript dataset and the preprocessing required to generate helpful ML model predictions. However, I learned a lot in this project and developed solutions to the challenges discussed in the project proposal: </span></p><p class="c1"><span class="c0">&#9679; I developed predictive machine learning models to classify whether views and completes a promotional offer based on personalized customer and offer characteristics. </span></p><p class="c1"><span class="c0">&#9679; Developed a predictive model to determine how much a customer spends by completing an offer after subtracting out the cost of the reward </span></p><p class="c1"><span class="c0">&#9679; The ML predictive model outperforms the benchmark model in each of our three studies, and the benchmark models also perform pretty well. </span></p><p class="c1"><span class="c0">&#9679; Determined the features that are most significant to whether or not a customer views and completes a promotional offer </span></p><p class="c1"><span class="c0">&#9679; Defined key customer segment groups based on customer demographics </span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The models and insights developed in this project allow the company to understand the primary drivers of an effective and profitable promotional offer. The predictive models will enable the company to personalize the characteristics of each promotion to maximize its odds of success for each customer. This project is an excellent example of how machine learning models benefit marketing campaigns and create business value. Lastly, this project shows why a more efficient and straightforward machine learning model is often preferable to a larger and more complex deep learning model.</span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.1qgla835x5gz"><span class="c7">Data Exploration </span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The structure of the dataset provided Starbucks Capstone project notebook is structured as follows. Three files contain the data: </span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">&#9679; portfolio.json - containing offer ids and metadata about each offer (duration, type, etc.) (See Fig. 1) </span></p><p class="c1"><span class="c0">&#9679; profile.json - demographic data for each customer (See Fig. 2) </span></p><p class="c1"><span class="c0">&#9679; transcript.json - records for transactions, offers received, offers viewed, and offers completed (See Fig. 3) </span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The three types of offers presented in the _type column of portfolio.json are: </span></p><p class="c1"><span class="c0">&#9679; Buy-one-get-one (BOGO): a user needs to spend a certain amount to get a reward equal to that threshold amount. </span></p><p class="c1"><span class="c0">&#9679; Discount: a user gains a reward equal to a fraction of the amount spent. </span></p><p class="c1"><span class="c0">&#9679; Informational offer: there is no reward, but neither is there a required amount that the user is expected to spend.</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.ittcl070uax8"><span class="c3">Regularization</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image22.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Regularization is a technique in machine learning that helps to prevent models from overfitting the training data, which can lead to poor generalization to new, unseen data. Overfitting occurs when a model learns the noise or random fluctuations in the training data instead of the actual underlying patterns. Regularization addresses this by adding a penalty to the loss function that the model optimizes. Here are the key types of regularization:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">1. **L1 Regularization (Lasso)**: Adds a penalty equal to the absolute value of the magnitude of coefficients. This can lead not only to smaller coefficients but can actually drive some coefficients to zero, thus performing variable selection. This is particularly useful when you suspect some features might be irrelevant to the output.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **L2 Regularization (Ridge)**: Adds a penalty equal to the square of the magnitude of coefficients. This tends to drive the coefficients to small values but not exactly zero. This is useful when you believe that all features have some influence on the output.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">3. **Elastic Net**: Combines L1 and L2 regularization. It adds both penalties (absolute value and square of the coefficients) to the loss function. This method is particularly useful when there are correlations among the features, as it can maintain the group effect of variables.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Regularization parameters are hyperparameters that control the strength of the penalty. The values of these parameters are typically determined through hyperparameter tuning methods like cross-validation.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The choice of regularization can depend on the particularities of the dataset and the problem:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">- **L1 regularization** is useful when trying to reduce the complexity of a model by eliminating some features entirely, which can be beneficial for models with high dimensionality.</span></p><p class="c1"><span class="c0">- **L2 regularization** is generally good at dealing with multicollinearity and model overfitting but doesn&#39;t reduce the number of features, as it generally keeps all features in the model but reduces the magnitude of the coefficients.</span></p><p class="c1"><span class="c0">- **Elastic Net** is best when there is a mix of features, some of which are important and others that are redundant or less important.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Regularization techniques can be applied across many types of models, including linear/logistic regression, neural networks, and even tree-based methods, although they manifest differently depending on the model architecture.</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.1f2ol3htst56"><span class="c3">Data Preprocessing</span></h1><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.ng86rddgmdib"><span class="c7">Normalization</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Data normalization is a crucial step in the data cleaning process for machine learning models for several key reasons:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">1. **Feature Scaling**: Many machine learning algorithms, such as those involving gradient descent (like linear regression and neural networks), or those that use distance calculations (like k-means clustering and k-nearest neighbors), perform better when the feature data are scaled to a standard range, typically 0 to 1 or -1 to 1. Normalization ensures that each feature contributes equally to the distance computations, preventing features with larger scales from dominating the distance calculations.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **Speeds Up Learning**: Normalization can help in speeding up the learning process by helping the optimization algorithms converge more quickly. When features are on different scales, the gradients may oscillate back and forth and take longer paths towards convergence, slowing down the training process.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">3. **Avoids Numerical Instability**: Algorithms can become numerically unstable due to very high or very low values. Normalization helps in avoiding these instabilities by ensuring the numbers stay within a range that is more manageable for the system.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">4. **Improves Algorithm Performance**: Some algorithms, particularly those that assume data is normally distributed, such as logistic regression, can perform better if the features are normalized.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">5. **Consistency**: When combining data from different sources, normalization is important to ensure that all data is on the same scale. This consistency is crucial when training a model to avoid biases toward one source or another due to scale differences.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Normalization, therefore, plays a pivotal role in preparing data for machine learning and ensuring that the predictive models are accurate, efficient, and robust.</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.rqnbrdu0iuyv"><span class="c3">Logistic regression</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Is logistic regression a misnomer? (Yes, because it is not regression, but classification based on regression)</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://github.com/jman4162/machine-learning-review/blob/main/Review_Python_Tutorial_on_Logistic_Regression.ipynb&amp;sa=D&amp;source=editors&amp;ust=1715648942490150&amp;usg=AOvVaw3nD7e00J1vMulyI-DyzjZQ">https://github.com/jman4162/machine-learning-review/blob/main/Review_Python_Tutorial_on_Logistic_Regression.ipynb</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c16">Logistic regression is a statistical model used for binary classification tasks, which predicts the probability that an observation falls into one of two categories. This makes it particularly useful in fields like medicine, finance, and social sciences for outcomes like disease/no disease, default/no default, etc. Unlike linear regression, logistic regression outputs probabilities by applying a logistic function to a linear combination of features.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">A logit is the natural logarithm of odds, or the log of the odds that Y=1. It&#39;s a fundamental concept in machine learning, and is used in binary and multi-class classification problems.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 310.00px; height: 70.00px;"><img alt="" src="images/image12.png" style="width: 310.00px; height: 70.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c16"></span></p><h1 class="c2" id="h.5ygzgo3b6ikl"><span class="c3">Support Vector Machine (SVM)</span></h1><p class="c1 c4"><span class="c16"></span></p><p class="c1"><span class="c18 c44"><a class="c8" href="https://www.google.com/url?q=https://github.com/jman4162/machine-learning-review/blob/main/Python_Review_Tutorial_Support_Vector_Machines_(SVM).ipynb&amp;sa=D&amp;source=editors&amp;ust=1715648942490770&amp;usg=AOvVaw213wtP48JPDl30ipAWPjkg">https://github.com/jman4162/machine-learning-review/blob/main/Python_Review_Tutorial_Support_Vector_Machines_(SVM).ipynb</a></span></p><p class="c1 c4"><span class="c16"></span></p><p class="c27"><span class="c16">Support Vector Machines (SVM) are a set of supervised learning methods used for classification, regression, and outliers detection. The advantages of support vector machines are:</span></p><ul class="c29 lst-kix_lwxg0pny7gv0-0 start"><li class="c38 c6 li-bullet-0"><span class="c16">Effective in high dimensional spaces.</span></li><li class="c6 c38 li-bullet-0"><span class="c16">Still effective in cases where the number of dimensions is greater than the number of samples.</span></li><li class="c38 c6 li-bullet-0"><span class="c16">Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</span></li></ul><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1227.00px; height: 756.00px;"><img alt="" src="images/image5.png" style="width: 1227.00px; height: 756.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c2" id="h.z98b4s4ufp83"><span class="c3">Convolutional Neural Networks (CNNs)</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image18.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c2 c34" id="h.e9qilfyuxwco"><span class="c0"></span></h1><h1 class="c2" id="h.5uzd3ty06cvd"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image17.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><h1 class="c2 c34" id="h.xrm1b68rfgir"><span class="c0"></span></h1><h2 class="c13" id="h.xrm1b68rfgir-1"><span class="c7">Transposed Convolutional Layer</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">A convolutional layer and a transposed convolutional layer (also known as a deconvolutional layer or a fractionally strided convolutional layer) are both fundamental components in convolutional neural networks (CNNs), but they serve different purposes and operate differently.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://chat.openai.com/share/ac406ba1-6dab-457b-8339-74a82ac919b9&amp;sa=D&amp;source=editors&amp;ust=1715648942491792&amp;usg=AOvVaw3fig5-yGOcqRkFdFIZb_op">https://chat.openai.com/share/ac406ba1-6dab-457b-8339-74a82ac919b9</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Used in GAN</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">In summary, convolutional layers are primarily used for downsampling and feature extraction, whereas transposed convolutional layers are used for upsampling and reconstructing images to higher resolutions.</span></p><h1 class="c2" id="h.xrm1b68rfgir-2"><span class="c3">Ensemble Learning </span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image2.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c12" id="h.n72m0d20ts5l"><span class="c7"></span></h2><p class="c1"><span class="c40">Decision trees, random forest and gradient boosting are all algorithms based on decision trees. &nbsp;There are many variants of decision trees, but they all do the same thing &ndash; subdivide the feature space into regions with mostly the same label. Decision trees are easy to understand and implement. However, they tend to over-fit data when we exhaust the branches and go very deep with the trees. Random Forrest and gradient boosting are two popular ways to use tree algorithms to achieve good accuracy as well as overcoming the over-fitting problem.</span></p><h2 class="c12" id="h.pngmvy72gt11"><span class="c7"></span></h2><h2 class="c13" id="h.2ht4egk6v13d"><span class="c7">Boosting Methods</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">In machine learning, boosting methods are ensemble techniques that build strong models by combining multiple weak learners. Here are a few commonly used boosting methods:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">1. **AdaBoost (Adaptive Boosting)**: One of the first and most popular boosting techniques, it adjusts the weights of incorrectly classified instances so that subsequent classifiers focus more on difficult cases.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">2. **Gradient Boosting**: This method builds models sequentially, each new model correcting errors made by the previous ones. The model adjustments are made by fitting the new model to the gradient of the loss function used for the training.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">3. **XGBoost (Extreme Gradient Boosting)**: An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">4. **LightGBM (Light Gradient Boosting Machine)**: A gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with faster training speed and lower memory usage.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">5. **CatBoost (Category Boosting)**: An algorithm that handles categorical features automatically and is robust to different types of data and overfitting. It provides a state-of-the-art performance with categorical data.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">These methods are widely used across various domains for tasks like classification, regression, and ranking due to their effectiveness in improving prediction accuracy.</span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.bg4ege20g4g6"><span class="c7">Stacking Methods</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Model stacking is a powerful ensemble technique used to improve the performance of machine learning models by combining multiple models to form a final prediction. The general approach to model stacking involves multiple layers of models, where each layer learns to correct the mistakes of the previous one. Here&rsquo;s a step-by-step breakdown of how model stacking is typically performed:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Step 1: Split the Data</span></p><p class="c1"><span class="c0">The dataset is divided into at least two subsets: a training set and a holdout set. In more sophisticated setups, you might use K-fold cross-validation to create multiple training and validation folds.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Step 2: Train Base Models</span></p><p class="c1"><span class="c0">Multiple base models (also known as level-0 models) are trained independently on the training set. These models can be of different types, including (but not limited to) decision trees, neural networks, SVMs, and more. The key is diversity among the models to capture various patterns in the data.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Step 3: Generate Meta-features</span></p><p class="c1"><span class="c0">The base models are used to predict the outcomes for the validation set (or each fold in cross-validation). The predictions from these base models serve as meta-features for the next layer. This means that instead of the original features, the inputs for the next model layer are the predictions made by the base models.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Step 4: Train a Meta-model</span></p><p class="c1"><span class="c0">A new model, known as the meta-model or level-1 model, is trained on the meta-features. The goal of this meta-model is to learn how best to combine the predictions of the base models to make a final prediction. This model essentially learns the strengths and weaknesses of the base models based on how well they perform on the validation set.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Step 5: Model Evaluation</span></p><p class="c1"><span class="c0">Finally, the meta-model is evaluated using a separate test set (the holdout set), which was not used during the training of base models and the meta-model. This helps in assessing how well the stacking model generalizes on unseen data.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Optional: Multiple Layers</span></p><p class="c1"><span class="c0">Although not always necessary, some stacking approaches may include more than one layer of meta-models, forming a deeper stack. Each additional layer aims to further refine the predictions made by the previous layer&rsquo;s models.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">### Final Prediction</span></p><p class="c1"><span class="c0">The final predictions are made by the meta-model, which uses the base models&rsquo; predictions as inputs. This approach often leads to better predictive performance compared to any single model or simple averaging techniques, as it effectively leverages the strengths of multiple models while mitigating their individual weaknesses.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Stacking is particularly useful in competitions like those on Kaggle, where small improvements in predictive accuracy can be crucial. It&#39;s a flexible approach, allowing for various configurations and adaptations based on specific needs and constraints of the problem at hand.</span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.4onw8vmtumih"><span class="c3">Clustering</span></h1><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.hyft0ehgkt4j"><span class="c7">K-means and GMM</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1227.00px; height: 427.00px;"><img alt="" src="images/image13.png" style="width: 1227.00px; height: 427.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><h2 class="c13" id="h.n5mezrbt4o3q"><span class="c7">Gaussian Mixture Model (GMM)</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://chat.openai.com/share/4f02d619-68a9-460b-991d-4bfe34011143&amp;sa=D&amp;source=editors&amp;ust=1715648942495055&amp;usg=AOvVaw1zrN4nsli7U61AQ3f-dzPy">https://chat.openai.com/share/4f02d619-68a9-460b-991d-4bfe34011143</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Gaussian Mixture Models (GMMs) are probabilistic models used for representing normally distributed subpopulations within an overall population. They are commonly used in statistical modeling, pattern recognition, and machine learning for clustering and density estimation. Here&#39;s a detailed explanation of GMMs and their training process:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Gaussian Mixture Model</span></p><p class="c1"><span>A GMM assumes that the data points are generated from a mixture of several Gaussian distributions, each with its own mean and covariance. The probability density function (PDF) of a GMM is a weighted sum of the PDFs of multiple Gaussian components:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Uses EM to train</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Applications of GMMs</span></p><ul class="c29 lst-kix_r4u3hmhmynip-0 start"><li class="c1 c6 li-bullet-0"><span class="c0">Clustering: Grouping data into clusters, where each cluster is modeled by a Gaussian distribution.</span></li><li class="c1 c6 li-bullet-0"><span class="c0">Density Estimation: Estimating the underlying probability distribution of a dataset.</span></li><li class="c1 c6 li-bullet-0"><span class="c0">Anomaly Detection: Identifying data points that do not fit the learned model well.</span></li><li class="c1 c6 li-bullet-0"><span class="c0">Image and Speech Processing: Modeling and classifying different segments or features.</span></li></ul><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.wsgkle3atc5y"><span class="c3">Loss functions</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The key takeaway is that the loss function is a measurable way to gauge the performance and accuracy of a machine learning model. In this case, the loss function acts as a guide for the learning process within a model or machine learning algorithm.</span></p><p class="c1"><span class="c0">The role of the loss function is crucial in the training of machine learning models and includes the following:</span></p><ul class="c29 lst-kix_f3tv83bt086d-0 start"><li class="c1 c6 li-bullet-0"><span class="c0">Performance measurement: Loss functions offer a clear metric to evaluate a model&#39;s performance by quantifying the difference between predictions and actual results.</span></li><li class="c1 c6 li-bullet-0"><span class="c0">Direction for improvement: Loss functions guide model improvement by directing the algorithm to adjust parameters(weights) iteratively to reduce loss and improve predictions.</span></li><li class="c1 c6 li-bullet-0"><span class="c0">Balancing bias and variance: Effective loss functions help balance model bias (oversimplification) and variance (overfitting), essential for the model&#39;s generalization to new data.</span></li><li class="c1 c6 li-bullet-0"><span>Influencing model behavior: Certain loss functions can affect the model&#39;s behavior, such as being more robust against data outliers or prioritizing specific types of errors.</span></li></ul><p class="c27 c4"><span class="c10"></span></p><p class="c27"><span class="c10">Let&#39;s explore the roles of particular loss functions in later sections and build a detailed intuition and understanding of the loss function.</span></p><p class="c1"><span>Source: </span><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://www.datacamp.com/tutorial/loss-function-in-machine-learning%23&amp;sa=D&amp;source=editors&amp;ust=1715648942497467&amp;usg=AOvVaw1F-GthBpWunsV400sI7c6P">https://www.datacamp.com/tutorial/loss-function-in-machine-learning#</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1426.00px; height: 1402.00px;"><img alt="" src="images/image1.png" style="width: 1426.00px; height: 1402.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">More: </span></p><ul class="c29 lst-kix_8w6oy4jdroww-0 start"><li class="c1 c6 li-bullet-0"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://neptune.ai/blog/pytorch-loss-functions&amp;sa=D&amp;source=editors&amp;ust=1715648942498022&amp;usg=AOvVaw0dR2z5_XgJHB7007Hq-a4u">https://neptune.ai/blog/pytorch-loss-functions</a></span></li><li class="c1 c6 li-bullet-0"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://pytorch.org/docs/stable/nn.html%23loss-functions&amp;sa=D&amp;source=editors&amp;ust=1715648942498212&amp;usg=AOvVaw31R1nNhGedLAaEok_XCoM4">https://pytorch.org/docs/stable/nn.html#loss-functions</a></span></li></ul><p class="c1"><span class="c0">&nbsp;</span></p><p class="c1"><span class="c0">Log-loss function</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span>Log-loss and cross-entropy loss: </span><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://chatgpt.com/share/fdc27e3c-c702-4e24-9e26-8890cdfdba69?oai-dm%3D1&amp;sa=D&amp;source=editors&amp;ust=1715648942498585&amp;usg=AOvVaw3QeST3TcRD48zKH_AWp3Do">https://chatgpt.com/share/fdc27e3c-c702-4e24-9e26-8890cdfdba69?oai-dm=1</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span>Log loss function: </span><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://scikit-learn.org/stable/modules/model_evaluation.html%23log-loss&amp;sa=D&amp;source=editors&amp;ust=1715648942498838&amp;usg=AOvVaw136RgqCZDdmQJ7Xzezb2LK">https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss</a></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1910.00px; height: 1402.00px;"><img alt="" src="images/image19.png" style="width: 1910.00px; height: 1402.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c13" id="h.ixfn7tb3wxp9"><span class="c7">ML Training Process</span></h2><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1321.00px; height: 859.00px;"><img alt="" src="images/image15.png" style="width: 1321.00px; height: 859.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c2" id="h.e46es6k5yoyd"><span class="c3">Autoencoder and Variational Autoencoder</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 720.00px; height: 932.00px;"><img alt="" src="images/image21.png" style="width: 720.00px; height: 932.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c2" id="h.q9umq9eix7ji"><span class="c3">Model Search Diagrams</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 2048.00px; height: 1277.00px;"><img alt="" src="images/image4.png" style="width: 2048.00px; height: 1277.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1152.00px; height: 648.00px;"><img alt="" src="images/image10.png" style="width: 1152.00px; height: 648.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c2" id="h.20wvyzwnf5x6"><span class="c3">Types of Machine Learning (ML) Algorithms</span></h1><h1 class="c2" id="h.pnnw21ui4ns8"><span class="c3">&nbsp;</span></h1><p class="c11"><span class="c0">This section provides an overview of the most popular types of machine learning. If you&rsquo;re familiar with these categories and want to move on to discussing specific algorithms, you can skip this section and go to &ldquo;When to use specific algorithms&rdquo; below.</span></p><h2 class="c23 c26" id="h.h1w6mukib5r1"><span class="c7">Supervised learning</span></h2><p class="c11"><span class="c0">Supervised learning algorithms make predictions based on a set of examples. For example, historical sales can be used to estimate future prices. With supervised learning, you have an input variable that consists of labeled training data and a desired output variable. You use an algorithm to analyze the training data to learn the function that maps the input to the output. This inferred function maps new, unknown examples by generalizing from the training data to anticipate results in unseen situations.</span></p><ul class="c29 lst-kix_lxfzxemgo2e9-0 start"><li class="c25 li-bullet-0"><span class="c32">Classification: </span><span class="c0">When the data are being used to predict a categorical variable, supervised learning is also called classification. This is the case when assigning a label or indicator, either dog or cat to an image. When there are only two labels, this is called binary classification. When there are more than two categories, the problems are called multi-class classification.</span></li><li class="c25 li-bullet-0"><span class="c32">Regression: </span><span class="c0">When predicting continuous values, the problems become a regression problem.</span></li><li class="c25 li-bullet-0"><span class="c32">Forecasting: </span><span class="c0">This is the process of making predictions about the future based on past and present data. It is most commonly used to analyze trends. A common example might be an estimation of the next year sales based on the sales of the current year and previous years.</span></li></ul><h2 class="c23 c26" id="h.cop2pnqpjem6"><span class="c7">Semi-supervised learning</span></h2><p class="c11"><span class="c0">The challenge with supervised learning is that labeling data can be expensive and time-consuming. If labels are limited, you can use unlabeled examples to enhance supervised learning. Because the machine is not fully supervised in this case, we say the machine is semi-supervised. With semi-supervised learning, you use unlabeled examples with a small amount of labeled data to improve the learning accuracy.</span></p><h2 class="c23 c26" id="h.tinjqhfui3jn"><span class="c7">Unsupervised learning</span></h2><p class="c11"><span class="c0">When performing unsupervised learning, the machine is presented with totally unlabeled data. It is asked to discover the intrinsic patterns that underlie the data, such as a clustering structure, a low-dimensional manifold, or a sparse tree and graph.</span></p><ul class="c29 lst-kix_r8bjmlao7ho2-0 start"><li class="c25 li-bullet-0"><span class="c32">Clustering: </span><span class="c0">Grouping a set of data examples so that examples in one group (or one cluster) are more similar (according to some criteria) than those in other groups. This is often used to segment the whole dataset into several groups. Analysis can be performed in each group to help users to find intrinsic patterns.</span></li><li class="c25 li-bullet-0"><span class="c32">Dimension reduction: </span><span class="c0">Reducing the number of variables under consideration. In many applications, the raw data have very high dimensional features and some features are redundant or irrelevant to the task. Reducing the dimensionality helps to find the true, latent relationship.</span></li></ul><h2 class="c23 c26" id="h.8kax47orvili"><span class="c7">&nbsp;Reinforcement learning</span></h2><p class="c11"><span>Reinforcement learning is another branch of machine learning which is mainly utilized for sequential decision-making problems. In this type of machine learning, unlike supervised and unsupervised learning, we do not need to have any data in advance; instead, the learning agent interacts with an environment and learns the optimal policy on the fly based on the feedback it receives from that environment. Specifically, in each time step, an agent observes the environment&rsquo;s state, chooses an action, and observes the feedback it receives from the environment. The feedback from an agent&rsquo;s action has many important components. One component is the resulting state of the environment after the agent has acted on it. Another component is the reward (or punishment) that the agent receives from performing that particular action in that particular state. The reward is carefully chosen to align with the objective for which we are training the agent. Using the state and reward, the agent updates its decision-making policy to optimize its long-term reward. With the recent advancements of </span><span class="c20"><a class="c8" href="https://www.google.com/url?q=https://www.sas.com/en_us/insights/analytics/deep-learning.html&amp;sa=D&amp;source=editors&amp;ust=1715648942502228&amp;usg=AOvVaw2bFzgUM7XWBd3hpr4BtnR-">deep learning</a></span><span>, reinforcement learning gained significant attention since it demonstrated striking performances in a wide range of applications such as games, robotics, and control. To see reinforcement learning models such as Deep-Q and Fitted-Q networks in action, check out </span><span class="c20"><a class="c8" href="https://www.google.com/url?q=https://blogs.sas.com/content/subconsciousmusings/2021/03/18/reinforcement-learning-deep-q-networks/&amp;sa=D&amp;source=editors&amp;ust=1715648942502421&amp;usg=AOvVaw2QJ3E3ugVNrxPARWDSVPqT">this article</a></span><span class="c0">.</span></p><h1 class="c2" id="h.1ge18u78oocv"><span class="c3">Resources </span></h1><p class="c1 c4"><span class="c0"></span></p><ul class="c29 lst-kix_c11g73hedk19-0 start"><li class="c1 c6 li-bullet-0"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://medium.com/swlh/cheat-sheets-for-machine-learning-interview-topics-51c2bc2bab4f&amp;sa=D&amp;source=editors&amp;ust=1715648942502961&amp;usg=AOvVaw2MzB9Ybpvjr6aQ09PGtnKi">https://medium.com/swlh/cheat-sheets-for-machine-learning-interview-topics-51c2bc2bab4f</a></span></li><li class="c1 c6 li-bullet-0"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://github.com/jman4162/machine-learning-review&amp;sa=D&amp;source=editors&amp;ust=1715648942503168&amp;usg=AOvVaw3l7DJWVhrF1iU9mKjHaVOK">https://github.com/jman4162/machine-learning-review</a></span></li><li class="c1 c6 li-bullet-0"><span class="c18"><a class="c8" href="https://www.google.com/url?q=https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/&amp;sa=D&amp;source=editors&amp;ust=1715648942503352&amp;usg=AOvVaw3Q0XxICw7Q3NJZfTF8OPS8">https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/</a></span></li><li class="c1 c4 c6 li-bullet-0"><span class="c0"></span></li></ul><p class="c1 c4"><span class="c0"></span></p></body></html>